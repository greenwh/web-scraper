# AI-Powered Web Scraper

A powerful Python tool for intelligent web scraping with two operational modes: **interactive browsing** and **automated data retrieval**.

## ğŸŒŸ Key Features

- **ğŸ¤– Multi-AI Support**: Works with Gemini, Claude, and OpenAI
- **ğŸŒ JavaScript Support**: Full browser automation via Playwright
- **ğŸ“¦ Offline Storage**: Save websites for offline analysis
- **ğŸ—ƒï¸ Database-Ready JSON**: Auto-generated schemas for easy database integration
- **ğŸ”„ Schema Reuse**: Use existing schemas for consistent parsing across runs
- **ğŸ¯ Smart Filtering**: Include/exclude URL patterns, domain restrictions
- **ğŸ“Š Structured Data**: Handles both textual and tabular content
- **ğŸ“ Auto Documentation**: Generates README with database examples
- **âš¡ Progress Tracking**: Resume interrupted crawls

## ğŸš€ Quick Start

**1. Install:**
```bash
# System dependencies (if needed)
sudo apt-get install libnspr4 libnss3 libasound2t64

# Python dependencies
pip install -r requirements.txt
playwright install chromium
```

**2. Set API Key:**
```bash
export GOOGLE_API_KEY="your-key"  # or ANTHROPIC_API_KEY or OPENAI_API_KEY
```

**3. Run:**
```bash
# Interactive mode
python main.py

# Data retrieval mode
python scrape_to_json.py https://example.com
```

ğŸ‘‰ **New user?** See [QUICKSTART.md](QUICKSTART.md) for detailed setup guide!

## ğŸ“‹ Two Modes of Operation

### Mode 1: Interactive Browsing ğŸ–±ï¸

Browse websites with real-time AI summaries:

```bash
python main.py
```

- Navigate through websites interactively
- Get AI summaries of each page
- Handle login-required sites
- Follow links or enter new URLs

### Mode 2: Data Retrieval & JSON Export ğŸ“Š

Crawl entire websites and export to structured JSON:

```bash
python scrape_to_json.py <url> [options]
```

**Perfect for:**
- Creating offline documentation databases
- Archiving medical/legal resources (e.g., SSA Blue Book)
- Extracting product catalogs
- Building searchable knowledge bases

## ğŸ¯ Primary Use Case: SSA Blue Book

Retrieve the complete Social Security Disability Blue Book as structured JSON:

```bash
python scrape_to_json.py \
    https://www.ssa.gov/disability/professionals/bluebook/AdultListings.htm \
    --max-depth 3 \
    --max-pages 200 \
    --output ssa_bluebook.json
```

**Output:** Database-ready JSON with:
- Impairment codes and categories
- Body system classifications
- Medical criteria and requirements
- Searchable and indexable structure

See `examples/ssa_bluebook_config.sh` for full configuration.

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| **[QUICKSTART.md](QUICKSTART.md)** | 5-minute setup guide for beginners |
| **[DATA_RETRIEVAL_README.md](DATA_RETRIEVAL_README.md)** | Complete guide for data retrieval mode |
| **[CLAUDE.md](CLAUDE.md)** | Technical architecture and API reference |

## ğŸ’¡ Usage Examples

### Example 1: Simple Documentation Site

```bash
python scrape_to_json.py https://docs.example.com \
    --max-depth 2 \
    --max-pages 50 \
    --include "/docs/"
```

### Example 2: Medical Guidelines with Claude AI

```bash
python scrape_to_json.py https://medical-guidelines.org \
    --provider claude \
    --max-depth 3 \
    --delay 2.0
```

### Example 3: Product Catalog

```bash
python scrape_to_json.py https://shop.example.com \
    --include "/products/" \
    --exclude "/cart/" \
    --max-pages 500
```

### Example 4: Crawl Only (No AI Conversion)

```bash
python scrape_to_json.py https://example.com \
    --skip-conversion \
    --max-pages 100
```

### Example 5: Reuse Schema for Consistent Updates

```bash
# First crawl - generate schema
python scrape_to_json.py https://example.com --output v1.json

# Later - reuse schema for consistent structure
python scrape_to_json.py https://example.com \
    --schema ./scraped_data/json/schema_analysis.json \
    --output v2.json
```

## ğŸ”§ Configuration Options

### Crawling Parameters

```bash
--max-depth 3           # How deep to follow links
--max-pages 100         # Maximum pages to crawl
--delay 2.0            # Seconds between requests
--same-domain          # Stay on same domain (default: true)
--include "/pattern/"  # Include URLs matching pattern
--exclude "/pattern/"  # Exclude URLs matching pattern
```

### AI Providers

```bash
--provider gemini      # Google Gemini (default, fast & cheap)
--provider claude      # Anthropic Claude (best for complex structures)
--provider openai      # OpenAI GPT (balanced)
--provider grok        # xAI Grok (alternative option)
--schema file.json     # Use existing schema for consistent parsing
```

### Output Options

```bash
--output file.json     # Output filename
--output-dir ./data    # Output directory
--skip-conversion      # Skip AI conversion (raw data only)
```

**See full options:** `python scrape_to_json.py --help`

## ğŸ“‚ Output Structure

```
scraped_data/
â”œâ”€â”€ html/                          # Raw HTML files
â”‚   â”œâ”€â”€ abc123.html
â”‚   â””â”€â”€ def456.html
â”œâ”€â”€ json/
â”‚   â”œâ”€â”€ scraped_data.json         # Raw extracted data
â”‚   â”œâ”€â”€ schema_analysis.json      # AI-generated schema
â”‚   â””â”€â”€ crawl_progress.json       # Progress tracking
â”œâ”€â”€ structured_data_*.json         # â­ YOUR FINAL DATABASE-READY JSON
â””â”€â”€ README.md                      # Auto-generated documentation
```

## ğŸ’¾ Database Integration

The tool generates database-ready JSON. Here's how to load it:

### MongoDB
```javascript
const data = require('./structured_data.json');
await db.collection.insertMany(data);
```

### PostgreSQL (JSONB)
```sql
CREATE TABLE my_data (id SERIAL, data JSONB);
-- Load using COPY or pg_import
```

### SQLite
```python
import json, sqlite3
conn = sqlite3.connect('mydb.db')
data = json.load(open('structured_data.json'))
# Insert data...
```

**Full examples** in the auto-generated README.md in your output directory!

## ğŸ§ª Testing

Run the test suite to verify installation:

```bash
python test_modules.py
```

## ğŸ“ Support

- ğŸ“– **Full Documentation**: See [DATA_RETRIEVAL_README.md](DATA_RETRIEVAL_README.md)
- ğŸš€ **Quick Start**: See [QUICKSTART.md](QUICKSTART.md)
- ğŸ—ï¸ **Technical Details**: See [CLAUDE.md](CLAUDE.md)

---

**Ready to get started?** â†’ [QUICKSTART.md](QUICKSTART.md)